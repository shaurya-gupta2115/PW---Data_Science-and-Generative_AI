{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Boosting Techniques Assignment - DA-AG-015\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Boosting is an ensemble learning method in machine learning that combines multiple weak learners sequentially to create a strong learner. It works on the principle that combining many simple models (weak learners) can result in a powerful and accurate model.\n",
        "\n",
        "### Key Concepts of Boosting:\n",
        "\n",
        "1. **Weak Learners**: These are models that perform slightly better than random chance. A common example is a decision stump (a decision tree with only one split).\n",
        "\n",
        "2. **Sequential Training**: Unlike bagging where models are trained in parallel, boosting trains models one after another. Each new model focuses on correcting the errors made by the previous models.\n",
        "\n",
        "3. **Weighted Training**: During each iteration, boosting assigns higher weights to misclassified examples and lower weights to correctly classified ones. This forces subsequent models to focus on the \"hard\" cases.\n",
        "\n",
        "### How Boosting Improves Weak Learners:\n",
        "\n",
        "- **Error Correction**: Each new weak learner is trained to correct the mistakes of the previous ensemble\n",
        "- **Weight Adjustment**: Misclassified instances get higher weights in the next iteration, making the algorithm focus on difficult cases\n",
        "- **Sequential Learning**: The combination of multiple weak learners results in a strong learner that can capture complex patterns\n",
        "- **Bias Reduction**: By focusing on errors, boosting effectively reduces bias in the final model\n",
        "\n",
        "The mathematical foundation shows that weak and strong learners are equivalent - any weak learning algorithm can be converted into a strong one through boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "### AdaBoost (Adaptive Boosting):\n",
        "\n",
        "**Training Process:**\n",
        "- **Data Reweighting**: AdaBoost modifies the training data distribution by adjusting sample weights\n",
        "- **Equal Initial Weights**: All training samples start with equal weights (1/n)\n",
        "- **Error-based Reweighting**: After each weak learner is trained, incorrectly classified samples get increased weights, correctly classified samples get decreased weights\n",
        "- **Weak Learner Weight**: Each weak learner gets a weight based on its error rate\n",
        "- **Focus**: Works primarily on the data samples (rows)\n",
        "\n",
        "### Gradient Boosting:\n",
        "\n",
        "**Training Process:**\n",
        "- **Residual Learning**: Instead of reweighting data, Gradient Boosting fits new models to the residuals (errors) of the previous ensemble\n",
        "- **Gradient Descent**: Uses gradient descent optimization to minimize a loss function\n",
        "- **Pseudo-residuals**: Each new model is trained on the negative gradients of the loss function\n",
        "- **Learning Rate**: Uses a shrinkage parameter (learning rate) to control the contribution of each model\n",
        "- **Focus**: Works on the loss function optimization\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "| Aspect | AdaBoost | Gradient Boosting |\n",
        "|--------|----------|-------------------|\n",
        "| **Data Handling** | Re-weights training samples | Fits to residuals/gradients |\n",
        "| **Loss Function** | Fixed (exponential loss) | Flexible (any differentiable loss) |\n",
        "| **Model Weights** | Based on error rates | Based on learning rate |\n",
        "| **Optimization** | Sample weight adjustment | Gradient descent |\n",
        "| **Generalization** | Specific algorithm | General framework |\n",
        "\n",
        "### Mathematical Difference:\n",
        "- **AdaBoost**: Weights are computed as exact solutions for exponential loss\n",
        "- **Gradient Boosting**: Uses gradients for any loss function, making it more flexible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Regularization in XGBoost is crucial for preventing overfitting and improving model generalization. XGBoost implements multiple regularization techniques that control model complexity.\n",
        "\n",
        "### Types of Regularization in XGBoost:\n",
        "\n",
        "#### 1. **L1 Regularization (Alpha Parameter)**\n",
        "- **Purpose**: Adds penalty based on absolute values of leaf weights\n",
        "- **Effect**: Encourages sparsity by driving some leaf weights to exactly zero\n",
        "- **Parameter**: `alpha` or `reg_alpha`\n",
        "- **Benefits**: Feature selection, simpler models, reduced overfitting\n",
        "\n",
        "#### 2. **L2 Regularization (Lambda Parameter)**\n",
        "- **Purpose**: Adds penalty based on squared values of leaf weights  \n",
        "- **Effect**: Smoothly shrinks leaf weights towards zero\n",
        "- **Parameter**: `lambda` or `reg_lambda`\n",
        "- **Benefits**: Smoother weight distribution, better generalization\n",
        "\n",
        "#### 3. **Tree-Specific Regularization**\n",
        "\n",
        "**Gamma (Min Split Loss):**\n",
        "- Controls minimum loss reduction required for node splitting\n",
        "- Higher gamma values → more conservative splits → simpler trees\n",
        "\n",
        "**Min Child Weight:**\n",
        "- Requires minimum sum of instance weights per leaf node\n",
        "- Prevents overly specific leaf nodes\n",
        "\n",
        "**Max Depth:**\n",
        "- Limits tree depth to control model complexity\n",
        "- Prevents trees from becoming too deep and overfitting\n",
        "\n",
        "#### 4. **Early Stopping**\n",
        "- **Parameter**: `early_stopping_rounds`\n",
        "- **Function**: Monitors validation metric and stops training when no improvement\n",
        "- **Benefit**: Finds optimal point before overfitting begins\n",
        "\n",
        "### How Regularization Helps:\n",
        "\n",
        "1. **Overfitting Prevention**: Penalizes complex models that fit noise\n",
        "2. **Generalization**: Improves performance on unseen data\n",
        "3. **Model Simplicity**: Creates more interpretable models\n",
        "4. **Computational Efficiency**: Reduces unnecessary complexity\n",
        "5. **Robust Performance**: Makes models less sensitive to training data variations\n",
        "\n",
        "### Mathematical Formulation:\n",
        "The XGBoost objective function includes regularization terms:\n",
        "\n",
        "```\n",
        "Obj = Σ L(yi, ŷi) + Σ Ω(ft)\n",
        "```\n",
        "\n",
        "Where:\n",
        "- L is the loss function\n",
        "- Ω is the regularization term = γT + (λ/2)Σw² + α Σ|w|\n",
        "- T is number of leaves, w are leaf weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "CatBoost is specifically designed to handle categorical features efficiently without requiring manual preprocessing. Here's why it excels:\n",
        "\n",
        "### 1. **Automatic Categorical Feature Handling**\n",
        "\n",
        "**No Preprocessing Required:**\n",
        "- CatBoost automatically detects and processes categorical features\n",
        "- No need for manual one-hot encoding or label encoding\n",
        "- Preserves original feature meaning and relationships\n",
        "\n",
        "**Built-in Encoding Methods:**\n",
        "- Uses sophisticated encoding techniques internally\n",
        "- Combines multiple encoding strategies for optimal performance\n",
        "\n",
        "### 2. **Advanced Encoding Techniques**\n",
        "\n",
        "**Target Statistics (CatBoost's Core Innovation):**\n",
        "- Calculates statistics based on target variable for each category\n",
        "- Uses historical data to avoid overfitting\n",
        "- Formula: `(countInClass + prior) / (totalCount + 1)`\n",
        "\n",
        "**Ordered Boosting:**\n",
        "- Uses random permutations of training data\n",
        "- Prevents target leakage during encoding\n",
        "- Ensures unbiased categorical feature processing\n",
        "\n",
        "### 3. **Handling High Cardinality**\n",
        "\n",
        "**Efficient Memory Usage:**\n",
        "- Handles features with thousands of categories\n",
        "- Uses optimized data structures\n",
        "- Doesn't explode dimensionality like one-hot encoding\n",
        "\n",
        "**Combination Features:**\n",
        "- Automatically creates combinations of categorical features\n",
        "- Discovers interaction patterns between categories\n",
        "- Builds more complex feature representations\n",
        "\n",
        "### 4. **Technical Advantages**\n",
        "\n",
        "**Missing Value Handling:**\n",
        "- Treats missing values as separate category\n",
        "- No need for imputation\n",
        "- Maintains data integrity\n",
        "\n",
        "**One-Hot Encoding Control:**\n",
        "- Uses one-hot encoding only for low-cardinality features\n",
        "- Parameter: `one_hot_max_size` (default varies by conditions)\n",
        "- Automatically chooses optimal encoding method\n",
        "\n",
        "### 5. **Performance Benefits**\n",
        "\n",
        "**Training Speed:**\n",
        "- Faster training compared to preprocessing + other algorithms\n",
        "- Optimized C++ implementation\n",
        "- GPU acceleration support\n",
        "\n",
        "**Model Quality:**\n",
        "- Better handling of categorical feature interactions\n",
        "- Reduced information loss\n",
        "- More accurate predictions on categorical-heavy datasets\n",
        "\n",
        "### Why This Matters:\n",
        "\n",
        "1. **Reduced Preprocessing Time**: No manual feature engineering needed\n",
        "2. **Better Feature Representation**: Preserves categorical relationships\n",
        "3. **Handling Complex Interactions**: Automatic feature combinations\n",
        "4. **Robust Performance**: Less prone to overfitting on categorical features\n",
        "5. **Industry Applications**: Excellent for domains with many categorical features (finance, e-commerce, healthcare)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Boosting techniques are preferred over bagging in several real-world scenarios where reducing bias and achieving high accuracy is crucial. Here are key applications:\n",
        "\n",
        "### 1. **Financial Services**\n",
        "\n",
        "**Credit Risk Assessment:**\n",
        "- **Why Boosting**: Need to identify subtle patterns in default prediction\n",
        "- **Algorithm Used**: XGBoost, LightGBM\n",
        "- **Advantage**: Sequential learning captures complex risk factors that individual models miss\n",
        "- **Real Impact**: Banks use boosting to reduce false positives/negatives in loan approvals\n",
        "\n",
        "**Fraud Detection:**\n",
        "- **Why Boosting**: Fraudulent transactions have subtle, evolving patterns\n",
        "- **Algorithm Used**: AdaBoost, Gradient Boosting\n",
        "- **Advantage**: Focuses on hard-to-detect fraudulent cases through iterative learning\n",
        "- **Real Impact**: PayPal, Visa use boosting for real-time fraud scoring\n",
        "\n",
        "### 2. **Healthcare and Medical Diagnosis**\n",
        "\n",
        "**Disease Prediction:**\n",
        "- **Why Boosting**: Medical diagnosis requires high precision on minority cases\n",
        "- **Algorithm Used**: XGBoost, CatBoost\n",
        "- **Advantage**: Excellent at handling imbalanced datasets (rare diseases)\n",
        "- **Real Impact**: Used for cancer detection, cardiovascular risk prediction\n",
        "\n",
        "### 3. **Computer Vision and Image Processing**\n",
        "\n",
        "**Object Detection (Viola-Jones Algorithm):**\n",
        "- **Why Boosting**: Real-time face detection requires speed and accuracy\n",
        "- **Algorithm Used**: AdaBoost with Haar features\n",
        "- **Advantage**: Creates strong classifier from simple rectangle features\n",
        "- **Real Impact**: Used in cameras, security systems, photo tagging\n",
        "\n",
        "### 4. **Search and Recommendation Systems**\n",
        "\n",
        "**Web Search Ranking:**\n",
        "- **Why Boosting**: Need to rank millions of pages accurately\n",
        "- **Algorithm Used**: Gradient Boosted Regression Trees (GBRT)\n",
        "- **Advantage**: Captures complex relevance signals\n",
        "- **Real Impact**: Google, Bing use boosting in their ranking algorithms\n",
        "\n",
        "### 5. **Marketing and Customer Analytics**\n",
        "\n",
        "**Customer Churn Prediction:**\n",
        "- **Why Boosting**: Early identification of at-risk customers\n",
        "- **Advantage**: Focuses on borderline cases that are most actionable\n",
        "- **Real Impact**: Telecom companies use boosting to reduce churn rates\n",
        "\n",
        "### When Boosting is Preferred Over Bagging:\n",
        "\n",
        "| Scenario | Why Boosting > Bagging |\n",
        "|----------|------------------------|\n",
        "| **High Bias Models** | Boosting reduces bias better than bagging |\n",
        "| **Imbalanced Data** | Sequential focus on minority class |\n",
        "| **Complex Patterns** | Better at capturing subtle interactions |\n",
        "| **High Accuracy Requirements** | Often achieves better precision/recall |\n",
        "| **Structured Data** | Excels on tabular data with mixed types |\n",
        "| **Real-time Scoring** | Can be optimized for fast inference |\n",
        "\n",
        "### Industry Examples:\n",
        "\n",
        "- **Microsoft**: Uses LightGBM for Bing search ranking\n",
        "- **Uber**: XGBoost for demand forecasting and pricing\n",
        "- **Airbnb**: Boosting models for pricing recommendations\n",
        "- **Pinterest**: CatBoost for content recommendation\n",
        "- **Spotify**: Gradient boosting for music recommendation\n",
        "\n",
        "The key insight is that boosting excels when you need to extract maximum predictive power from structured data and when the cost of misclassification is high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Practical Implementation Section\n",
        "\n",
        "Now let's move to the practical coding questions using the specified datasets:\n",
        "- **Classification tasks**: `sklearn.datasets.load_breast_cancer()`\n",
        "- **Regression tasks**: `sklearn.datasets.fetch_california_housing()`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 6: Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "**Task:**\n",
        "- Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "- Print the model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "print(\"Loading Breast Cancer Dataset...\")\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Target classes: {data.target_names}\")\n",
        "print(f\"Feature names: {data.feature_names[:5]}...\")  # Show first 5 features\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train AdaBoost Classifier\n",
        "print(\"Training AdaBoost Classifier...\")\n",
        "ada_classifier = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1),  # Decision stumps\n",
        "    n_estimators=50,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "ada_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ada_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n=== AdaBoost Classifier Results ===\")\n",
        "print(f\"Model Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = ada_classifier.feature_importances_\n",
        "top_features_idx = np.argsort(feature_importance)[-5:]\n",
        "print(\"\\nTop 5 Important Features:\")\n",
        "for idx in reversed(top_features_idx):\n",
        "    print(f\"{data.feature_names[idx]}: {feature_importance[idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 7: Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "**Task:**\n",
        "- Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "- Evaluate performance using R-squared score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the California Housing dataset\n",
        "print(\"Loading California Housing Dataset...\")\n",
        "housing_data = fetch_california_housing()\n",
        "X, y = housing_data.data, housing_data.target\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Target variable: House values in hundreds of thousands of dollars\")\n",
        "print(f\"Feature names: {housing_data.feature_names}\")\n",
        "print(f\"Target statistics: Mean={y.mean():.2f}, Std={y.std():.2f}, Min={y.min():.2f}, Max={y.max():.2f}\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train Gradient Boosting Regressor\n",
        "print(\"Training Gradient Boosting Regressor...\")\n",
        "gb_regressor = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n=== Gradient Boosting Regressor Results ===\")\n",
        "print(f\"R-squared Score: {r2:.4f} ({r2*100:.2f}%)\")\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
        "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = gb_regressor.feature_importances_\n",
        "feature_names = housing_data.feature_names\n",
        "\n",
        "print(\"\\nFeature Importance Ranking:\")\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "for idx, row in feature_importance_df.iterrows():\n",
        "    print(f\"{row['Feature']}: {row['Importance']:.4f}\")\n",
        "\n",
        "# Training progress\n",
        "print(f\"\\nModel Training Info:\")\n",
        "print(f\"Number of estimators used: {gb_regressor.n_estimators}\")\n",
        "print(f\"Training score: {gb_regressor.train_score_[-1]:.4f}\")\n",
        "\n",
        "# Create a simple prediction comparison\n",
        "print(\"\\nSample Predictions vs Actual:\")\n",
        "sample_indices = np.random.choice(len(y_test), 5, replace=False)\n",
        "print(\"Actual -> Predicted\")\n",
        "for idx in sample_indices:\n",
        "    print(f\"{y_test[idx]:.2f} -> {y_pred[idx]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 8: Train an XGBoost Classifier with GridSearchCV for learning rate tuning\n",
        "\n",
        "**Task:**\n",
        "- Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "- Tune the learning rate using GridSearchCV\n",
        "- Print the best parameters and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "print(\"Loading Breast Cancer Dataset for XGBoost...\")\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Training set: {X_train.shape[0]}, Test set: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create XGBoost Classifier\n",
        "print(\"Setting up XGBoost Classifier...\")\n",
        "xgb_classifier = XGBClassifier(\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "# Define parameter grid for learning rate tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [3, 6]\n",
        "}\n",
        "\n",
        "print(f\"Parameter grid: {param_grid}\")\n",
        "print(\"Starting GridSearchCV...\")\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_classifier,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the grid search\n",
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"=== GridSearchCV Results ===\")\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Cross-Validation Score: {best_score:.4f} ({best_score*100:.2f}%)\")\n",
        "\n",
        "# Train the best model on full training set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate final accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n=== Final XGBoost Model Performance ===\")\n",
        "print(f\"Test Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n",
        "\n",
        "# Detailed results\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "# Feature importance from best model\n",
        "feature_importance = best_model.feature_importances_\n",
        "top_features_idx = np.argsort(feature_importance)[-5:]\n",
        "\n",
        "print(\"\\nTop 5 Important Features:\")\n",
        "for idx in reversed(top_features_idx):\n",
        "    print(f\"{data.feature_names[idx]}: {feature_importance[idx]:.4f}\")\n",
        "\n",
        "# Show all parameter combinations tested\n",
        "print(\"\\nAll Parameter Combinations Tested:\")\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "for idx, params in enumerate(grid_search.cv_results_['params']):\n",
        "    score = grid_search.cv_results_['mean_test_score'][idx]\n",
        "    print(f\"{params} -> CV Score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 9: Train a CatBoost Classifier and plot confusion matrix\n",
        "\n",
        "**Task:**\n",
        "- Train a CatBoost Classifier\n",
        "- Plot the confusion matrix using seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "print(\"Loading Breast Cancer Dataset for CatBoost...\")\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Convert to DataFrame for better handling\n",
        "X_df = pd.DataFrame(X, columns=data.feature_names)\n",
        "\n",
        "print(f\"Dataset shape: {X_df.shape}\")\n",
        "print(f\"Target classes: {data.target_names}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_df, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]}, Test set: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train CatBoost Classifier\n",
        "print(\"Training CatBoost Classifier...\")\n",
        "catboost_classifier = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    depth=6,\n",
        "    learning_rate=0.1,\n",
        "    loss_function='Logloss',\n",
        "    verbose=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "catboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = catboost_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n=== CatBoost Classifier Results ===\")\n",
        "print(f\"Model Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, \n",
        "           annot=True, \n",
        "           fmt='d', \n",
        "           cmap='Blues',\n",
        "           xticklabels=data.target_names,\n",
        "           yticklabels=data.target_names,\n",
        "           cbar_kws={'label': 'Count'})\n",
        "\n",
        "plt.title('CatBoost Classifier - Confusion Matrix\\nBreast Cancer Dataset', fontsize=14, pad=20)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = catboost_classifier.feature_importances_\n",
        "top_features_idx = np.argsort(feature_importance)[-10:]\n",
        "\n",
        "print(\"\\nTop 10 Important Features:\")\n",
        "for idx in reversed(top_features_idx):\n",
        "    print(f\"{data.feature_names[idx]}: {feature_importance[idx]:.4f}\")\n",
        "\n",
        "# Additional metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n=== Additional Metrics ===\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Model parameters used\n",
        "print(f\"\\n=== Model Configuration ===\")\n",
        "print(f\"Iterations: {catboost_classifier.get_params()['iterations']}\")\n",
        "print(f\"Depth: {catboost_classifier.get_params()['depth']}\")\n",
        "print(f\"Learning Rate: {catboost_classifier.get_params()['learning_rate']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 10: Complete Data Science Pipeline for Loan Default Prediction\n",
        "\n",
        "**Task:**\n",
        "You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior. The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "- Data preprocessing & handling missing/categorical values\n",
        "- Choice between AdaBoost, XGBoost, or CatBoost\n",
        "- Hyperparameter tuning strategy\n",
        "- Evaluation metrics you'd choose and why\n",
        "- How the business would benefit from your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries for comprehensive pipeline\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (roc_auc_score, roc_curve, precision_recall_curve, \n",
        "                           f1_score, recall_score)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "print(\"=== FINTECH LOAN DEFAULT PREDICTION PIPELINE ===\")\n",
        "print(\"Using Boosting Techniques for Imbalanced Dataset\\n\")\n",
        "\n",
        "# STEP 1: CREATE SYNTHETIC REALISTIC LOAN DATASET\n",
        "print(\"STEP 1: Creating Synthetic Loan Dataset...\")\n",
        "\n",
        "np.random.seed(42)\n",
        "n_samples = 5000\n",
        "\n",
        "# Generate realistic loan data with missing values and mixed types\n",
        "data = {\n",
        "    'loan_amount': np.random.normal(25000, 15000, n_samples),\n",
        "    'annual_income': np.random.normal(50000, 25000, n_samples),\n",
        "    'credit_score': np.random.normal(650, 100, n_samples),\n",
        "    'employment_years': np.random.exponential(5, n_samples),\n",
        "    'debt_to_income': np.random.uniform(0.1, 0.8, n_samples),\n",
        "    'loan_term': np.random.choice([12, 24, 36, 48, 60], n_samples),\n",
        "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples),\n",
        "    'home_ownership': np.random.choice(['Rent', 'Own', 'Mortgage'], n_samples),\n",
        "    'loan_purpose': np.random.choice(['Debt Consolidation', 'Home Improvement', \n",
        "                                    'Medical', 'Business', 'Other'], n_samples),\n",
        "    'state': np.random.choice(['CA', 'NY', 'TX', 'FL', 'IL'], n_samples)\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create realistic default probability based on features\n",
        "default_prob = (\n",
        "    (df['debt_to_income'] > 0.5) * 0.3 +\n",
        "    (df['credit_score'] < 600) * 0.4 +\n",
        "    (df['annual_income'] < 30000) * 0.2 +\n",
        "    (df['employment_years'] < 2) * 0.15\n",
        ")\n",
        "\n",
        "# Create target variable (imbalanced - 15% default rate)\n",
        "df['default'] = np.random.binomial(1, np.clip(default_prob, 0.05, 0.6), n_samples)\n",
        "\n",
        "# Introduce missing values realistically\n",
        "missing_indices = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\n",
        "df.loc[missing_indices[:len(missing_indices)//2], 'credit_score'] = np.nan\n",
        "df.loc[missing_indices[len(missing_indices)//2:], 'employment_years'] = np.nan\n",
        "\n",
        "print(f\"Dataset created with {n_samples} samples\")\n",
        "print(f\"Default rate: {df['default'].mean():.1%}\")\n",
        "print(f\"Missing values: {df.isnull().sum().sum()} total\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2: DATA PREPROCESSING & FEATURE ENGINEERING\n",
        "print(\"STEP 2: Data Preprocessing...\")\n",
        "\n",
        "# Handle missing values\n",
        "print(\"Handling missing values...\")\n",
        "df['credit_score'].fillna(df['credit_score'].median(), inplace=True)\n",
        "df['employment_years'].fillna(df['employment_years'].median(), inplace=True)\n",
        "\n",
        "# Feature engineering\n",
        "print(\"Creating new features...\")\n",
        "df['loan_to_income_ratio'] = df['loan_amount'] / df['annual_income']\n",
        "df['credit_score_category'] = pd.cut(df['credit_score'], \n",
        "                                   bins=[0, 600, 700, 850], \n",
        "                                   labels=['Poor', 'Good', 'Excellent'])\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_features = ['education', 'home_ownership', 'loan_purpose', 'state', 'credit_score_category']\n",
        "label_encoders = {}\n",
        "\n",
        "df_processed = df.copy()\n",
        "for feature in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    df_processed[feature + '_encoded'] = le.fit_transform(df_processed[feature].astype(str))\n",
        "    label_encoders[feature] = le\n",
        "\n",
        "# Prepare features for modeling\n",
        "numerical_features = ['loan_amount', 'annual_income', 'credit_score', 'employment_years', \n",
        "                     'debt_to_income', 'loan_term', 'loan_to_income_ratio']\n",
        "encoded_features = [f + '_encoded' for f in categorical_features]\n",
        "\n",
        "all_features = numerical_features + encoded_features\n",
        "X = df_processed[all_features]\n",
        "y = df_processed['default']\n",
        "\n",
        "print(f\"Final feature set: {len(all_features)} features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3: TRAIN-TEST SPLIT & CLASS IMBALANCE\n",
        "print(\"STEP 3: Splitting data and addressing class imbalance...\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Training default rate: {y_train.mean():.1%}\")\n",
        "\n",
        "# Handle class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"Original training set: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Balanced training set: {pd.Series(y_train_balanced).value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 4: MODEL SELECTION AND COMPARISON\n",
        "print(\"STEP 4: Model Selection - Comparing Boosting Algorithms...\")\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0),\n",
        "    'CatBoost': CatBoostClassifier(random_state=42, verbose=False)\n",
        "}\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores = {}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(model, X_train_balanced, y_train_balanced, \n",
        "                           cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "    cv_scores[name] = scores\n",
        "    print(f\"{name} CV AUC: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
        "\n",
        "# Select best model based on CV scores\n",
        "best_model_name = max(cv_scores.keys(), key=lambda k: cv_scores[k].mean())\n",
        "print(f\"\\nBest model: {best_model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 5: HYPERPARAMETER TUNING\n",
        "print(f\"STEP 5: Hyperparameter Tuning for {best_model_name}...\")\n",
        "\n",
        "if best_model_name == 'XGBoost':\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100],\n",
        "        'learning_rate': [0.1, 0.2],\n",
        "        'max_depth': [3, 6],\n",
        "        'subsample': [0.8, 1.0]\n",
        "    }\n",
        "    best_model = XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0)\n",
        "    \n",
        "elif best_model_name == 'CatBoost':\n",
        "    param_grid = {\n",
        "        'iterations': [50, 100],\n",
        "        'learning_rate': [0.1, 0.2],\n",
        "        'depth': [4, 6]\n",
        "    }\n",
        "    best_model = CatBoostClassifier(random_state=42, verbose=False)\n",
        "    \n",
        "else:  # AdaBoost\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100],\n",
        "        'learning_rate': [0.5, 1.0, 1.5]\n",
        "    }\n",
        "    best_model = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(\n",
        "    best_model, param_grid, cv=3, scoring='roc_auc', n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV score: {grid_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 6: MODEL EVALUATION\n",
        "print(\"STEP 6: Final Model Evaluation...\")\n",
        "\n",
        "# Train best model\n",
        "final_model = grid_search.best_estimator_\n",
        "y_pred = final_model.predict(X_test)\n",
        "y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n=== FINAL MODEL PERFORMANCE ===\")\n",
        "print(f\"Model: {best_model_name}\")\n",
        "print(f\"AUC Score: {auc_score:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=['No Default', 'Default'],\n",
        "           yticklabels=['No Default', 'Default'])\n",
        "plt.title(f'{best_model_name} - Confusion Matrix\\nLoan Default Prediction')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'{best_model_name} (AUC = {auc_score:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Loan Default Prediction')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 7: BUSINESS IMPACT ANALYSIS\n",
        "print(\"STEP 7: Business Impact Analysis...\")\n",
        "\n",
        "# Calculate business metrics\n",
        "true_defaults = (y_test == 1).sum()\n",
        "predicted_defaults = (y_pred == 1).sum()\n",
        "correctly_identified_defaults = ((y_test == 1) & (y_pred == 1)).sum()\n",
        "\n",
        "# Assume average loan amount and loss given default\n",
        "avg_loan_amount = 25000\n",
        "loss_given_default = 0.6  # 60% loss rate on defaults\n",
        "\n",
        "# Business impact calculations\n",
        "total_exposure = len(y_test) * avg_loan_amount\n",
        "potential_losses = true_defaults * avg_loan_amount * loss_given_default\n",
        "prevented_losses = correctly_identified_defaults * avg_loan_amount * loss_given_default\n",
        "missed_losses = (true_defaults - correctly_identified_defaults) * avg_loan_amount * loss_given_default\n",
        "\n",
        "print(f\"\\n=== BUSINESS IMPACT ANALYSIS ===\")\n",
        "print(f\"Total loan portfolio exposure: ${total_exposure:,}\")\n",
        "print(f\"Actual defaults in test set: {true_defaults}\")\n",
        "print(f\"Predicted defaults: {predicted_defaults}\")\n",
        "print(f\"Correctly identified defaults: {correctly_identified_defaults}\")\n",
        "print(f\"Potential total losses: ${potential_losses:,}\")\n",
        "print(f\"Prevented losses: ${prevented_losses:,}\")\n",
        "print(f\"Missed losses: ${missed_losses:,}\")\n",
        "print(f\"Loss prevention rate: {prevented_losses/potential_losses:.1%}\")\n",
        "\n",
        "# Risk-based pricing recommendations\n",
        "high_risk_threshold = 0.3\n",
        "medium_risk_threshold = 0.15\n",
        "\n",
        "risk_categories = []\n",
        "for prob in y_pred_proba:\n",
        "    if prob >= high_risk_threshold:\n",
        "        risk_categories.append('High Risk')\n",
        "    elif prob >= medium_risk_threshold:\n",
        "        risk_categories.append('Medium Risk')\n",
        "    else:\n",
        "        risk_categories.append('Low Risk')\n",
        "\n",
        "risk_distribution = pd.Series(risk_categories).value_counts()\n",
        "print(f\"\\n=== RISK-BASED PORTFOLIO SEGMENTATION ===\")\n",
        "print(risk_distribution)\n",
        "\n",
        "print(f\"\\n=== MODEL DEPLOYMENT RECOMMENDATIONS ===\")\n",
        "print(\"1. Deploy model for real-time loan application scoring\")\n",
        "print(\"2. Implement risk-based pricing tiers\")\n",
        "print(\"3. Set up automated alerts for high-risk applications\")\n",
        "print(\"4. Regular model retraining (quarterly)\")\n",
        "print(\"5. Monitor model performance and drift\")\n",
        "print(f\"6. Expected ROI: ${prevented_losses:,} in prevented losses\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n=== PIPELINE SUMMARY ===\")\n",
        "print(f\"✓ Processed {n_samples} loan applications\")\n",
        "print(f\"✓ Handled missing values and categorical features\")\n",
        "print(f\"✓ Addressed class imbalance with SMOTE\")\n",
        "print(f\"✓ Compared multiple boosting algorithms\")\n",
        "print(f\"✓ Optimized hyperparameters\")\n",
        "print(f\"✓ Achieved {auc_score:.1%} AUC score\")\n",
        "print(f\"✓ Prevented ${prevented_losses:,} in potential losses\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
